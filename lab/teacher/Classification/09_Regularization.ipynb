{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23d10f4",
   "metadata": {},
   "source": [
    "# 加入正規化的成本函數與梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734bfca4",
   "metadata": {},
   "source": [
    "## 目標\n",
    "\n",
    "在這個實驗中，你將會：\n",
    "\n",
    "- 在先前的線性回歸與邏輯回歸成本函數中加入正規化（regularization）項。\n",
    "\n",
    "- 重新執行先前的過擬合（overfitting）範例，觀察加入正規化後的差異。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b54c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 資料載入\n",
    "import sys, os\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=8)\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    %matplotlib widget\n",
    "except:\n",
    "    %matplotlib inline\n",
    "    print(\"Colab not support matplotlib widget\")\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import Output\n",
    "\n",
    "\n",
    "#先檢查README,有的話表示是完整檔案不用下載\n",
    "def find_repo_root(marker=\"README.md\"):\n",
    "    cur = Path.cwd()\n",
    "    while cur != cur.parent:  # 防止無限迴圈，到達檔案系統根目錄就停\n",
    "        if (cur / marker).exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    return None\n",
    "\n",
    "\n",
    "def import_data_from_github():\n",
    "    import urllib.request, shutil\n",
    "    \n",
    "    def isRunningInColab() -> bool:\n",
    "        return \"google.colab\" in sys.modules\n",
    "\n",
    "    def isRunningInJupyterLab() -> bool:\n",
    "        try:\n",
    "            import jupyterlab\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "        \n",
    "    def detect_env():\n",
    "        from IPython import get_ipython\n",
    "        if isRunningInColab():\n",
    "            return \"Colab\"\n",
    "        elif isRunningInJupyterLab():\n",
    "            return \"JupyterLab\"\n",
    "        elif \"notebook\" in str(type(get_ipython())).lower():\n",
    "            return \"Jupyter Notebook\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "    def get_utils_dir(env): \n",
    "        if env == \"Colab\": \n",
    "            if \"/content\" not in sys.path:\n",
    "                sys.path.insert(0, \"/content\")\n",
    "            return \"/content/utils\"\n",
    "        else:\n",
    "            return Path.cwd() / \"utils\"\n",
    "\n",
    "    env = detect_env()\n",
    "    UTILS_DIR = get_utils_dir(env)\n",
    "    REPO_DIR = \"Machine-Learning-Lab\"\n",
    "\n",
    "    #shutil.rmtree(UTILS_DIR, ignore_errors=True)\n",
    "    os.makedirs(UTILS_DIR, exist_ok=True)\n",
    "\n",
    "    BASE = f\"https://raw.githubusercontent.com/mz038197/{REPO_DIR}/main\"\n",
    "    urllib.request.urlretrieve(f\"{BASE}/utils/plt_overfit.py\", f\"{UTILS_DIR}/plt_overfit.py\")\n",
    "    urllib.request.urlretrieve(f\"{BASE}/utils/lab_utils_common_classification.py\", f\"{UTILS_DIR}/lab_utils_common_classification.py\")\n",
    "    urllib.request.urlretrieve(f\"{BASE}/utils/deeplearning.mplstyle\", f\"{UTILS_DIR}/deeplearning.mplstyle\")\n",
    "\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "\n",
    "if repo_root is None:\n",
    "    import_data_from_github()\n",
    "    repo_root = Path.cwd()\n",
    "    \n",
    "\n",
    "os.chdir(repo_root)\n",
    "print(f\"✅ 切換工作目錄至 {Path.cwd()}\")\n",
    "sys.path.append(str(repo_root)) if str(repo_root) not in sys.path else None\n",
    "print(f\"✅ 加入到系統路徑\")\n",
    "\n",
    "from utils.plt_overfit import overfit_example, output\n",
    "from utils.lab_utils_common_classification import sigmoid\n",
    "\n",
    "\n",
    "plt.style.use('utils/deeplearning.mplstyle')\n",
    "print(\"✅ 匯入模組及設定繪圖樣式\")\n",
    "#endregion 資料載入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6bff3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4809341",
   "metadata": {},
   "source": [
    "# 加入正規化（Regularization）\n",
    "\n",
    "上面的投影片展示了線性回歸與邏輯回歸加入正規化後的**成本函數**與**梯度**。重點如下：\n",
    "- 成本（Cost）\n",
    "    - 線性回歸與邏輯回歸的成本函數形式差異很大，但「把正規化加進去」的方式是一樣的。\n",
    "- 梯度（Gradient）\n",
    "    - 線性回歸與邏輯回歸的梯度形式非常相近，主要差別只在於模型輸出 $f_{\\mathbf{w},b}$ 的計算方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236cf68f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360b82e",
   "metadata": {},
   "source": [
    "## 加入正規化的成本函數\n",
    "### 正規化的線性回歸成本函數\n",
    "\n",
    "正規化線性回歸（regularized linear regression）的成本函數為：\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{1}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$ \n",
    "f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{2} \n",
    "$$\n",
    "\n",
    "把它和「沒有正規化」的成本函數（你在前一個實驗已實作過）相比：\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \n",
    "$$\n",
    "\n",
    "差別在於多了一個正規化項：<span style=\"color:blue\"> $$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$ </span>\n",
    "\n",
    "加入這個項會鼓勵梯度下降同時把參數的「大小」也變小（避免權重過大）。注意：此範例中 **$b$ 不做正規化**，這是常見且標準的做法。\n",
    "\n",
    "下面提供式 (1) 與 (2) 的實作。這裡使用了本課程常見的寫法：用 `for loop` 逐筆走訪全部 `m` 筆資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ab3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    m  = X.shape[0]\n",
    "    n  = len(w)\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot\n",
    "        cost = cost + (f_wb_i - y[i])**2                               #scalar             \n",
    "    cost = cost / (2 * m)                                              #scalar  \n",
    " \n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          #scalar\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
    "    \n",
    "    total_cost = cost + reg_cost                                       #scalar\n",
    "    # YOUR CODE END HERE \n",
    "\n",
    "    return total_cost                                                  #scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0883324a",
   "metadata": {},
   "source": [
    "執行下方 cell 來看看實際效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3135d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d117e",
   "metadata": {},
   "source": [
    "**預期輸出**：\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>正規化後的 cost：</b> 0.07917239320214275 </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc18bd",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445289fb",
   "metadata": {},
   "source": [
    "### 正規化的邏輯回歸成本函數\n",
    "\n",
    "對於正規化的 **邏輯回歸（logistic regression）**，成本函數形式為：\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{3}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$ \n",
    "f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = sigmoid(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)  \\tag{4} \n",
    "$$ \n",
    "\n",
    "把它和「沒有正規化」的成本函數（你在前一個實驗已實作過）相比：\n",
    "\n",
    "$$ \n",
    "J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right] \n",
    "$$\n",
    "\n",
    "和上面的線性回歸一樣，差別在於多了正規化項：<span style=\"color:blue\"> $$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$ </span>\n",
    "\n",
    "加入這個項會鼓勵梯度下降去縮小參數的大小。注意：此範例中 **$b$ 不做正規化**，這是標準作法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7576002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    m,n  = X.shape\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot\n",
    "        f_wb_i = sigmoid(z_i)                                          #scalar\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar\n",
    "             \n",
    "    cost = cost/m                                                      #scalar\n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          #scalar\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
    "    \n",
    "    total_cost = cost + reg_cost                                       #scalar\n",
    "    # YOUR CODE END HERE \n",
    "    \n",
    "    return total_cost                                                  #scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e66e34",
   "metadata": {},
   "source": [
    "執行下方 cell 來看看實際效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de927cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115ed3a",
   "metadata": {},
   "source": [
    "**預期輸出**：\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>正規化後的 cost：</b> 0.6850849138741673 </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2b85d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加入正規化的梯度下降（Gradient Descent）\n",
    "梯度下降的基本流程在加入正規化後**不會改變**，仍然是：\n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace\n",
    "\\end{align*}$$\n",
    "其中每一次迭代都會對所有 $j$ 的 $w_j$ 進行**同步更新**。\n",
    "\n",
    "加入正規化後真正改變的是：**梯度（gradients）的計算方式**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c1afa",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加入正規化的梯度計算（線性/邏輯皆適用）\n",
    "\n",
    "線性回歸與邏輯回歸在計算梯度時幾乎一樣，差別主要只在於 $f_{\\mathbf{w},b}$ 的計算方式。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\tag{2} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- $m$：資料集中訓練樣本的數量\n",
    "\n",
    "- $f_{\\mathbf{w},b}(x^{(i)})$：模型的預測值，$y^{(i)}$：目標值（標籤）\n",
    "\n",
    "- 對於 <span style=\"color:blue\">**線性回歸**</span>：\n",
    "    $f_{\\mathbf{w},b}(x) = \\mathbf{w} \\cdot \\mathbf{x} + b$\n",
    "\n",
    "- 對於 <span style=\"color:blue\">**邏輯回歸**</span>：\n",
    "    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$\n",
    "    $f_{\\mathbf{w},b}(x) = g(z)$\n",
    "    其中 $g(z)$ 是 sigmoid 函數：\n",
    "    $g(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "加入正規化後，梯度中多出來的項是 <span style=\"color:blue\"> $$\\frac{\\lambda}{m} w_j$$ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff9259",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正規化的線性回歸梯度函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_linear_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]                 \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m   \n",
    "    \n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "    # YOUR CODE END HERE \n",
    "    \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "執行下方 cell 來看看實際效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**預期輸出**\n",
    "```\n",
    "dj_db: 0.6648774569425726\n",
    "正規化後的 dj_dw:\n",
    " [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d7186",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正規化的邏輯回歸梯度函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns\n",
    "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                            #(n,)\n",
    "    dj_db = 0.0                                       #scalar\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "    # YOUR CODE END HERE \n",
    "    \n",
    "    return dj_db, dj_dw  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "執行下方 cell 來看看實際效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**預期輸出**\n",
    "```\n",
    "dj_db: 0.341798994972791\n",
    "正規化後的 dj_dw:\n",
    " [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217cd15",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重新執行過擬合（Overfitting）範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "display(output)\n",
    "ofit = overfit_example(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的圖中，請在先前的例子上試試看「加入正規化」的效果。建議操作如下：\n",
    "- 分類（邏輯回歸）\n",
    "    - 將 degree 設為 6、lambda 設為 0（不正規化），進行擬合（fit）\n",
    "    - 接著把 lambda 設為 1（提高正規化強度），再擬合一次，觀察差異\n",
    "- 回歸（線性回歸）\n",
    "    - 用相同的步驟操作並比較結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 恭喜！\n",
    "你已經完成：\n",
    "- 線性回歸與邏輯回歸加入正規化後的成本函數與梯度計算範例\n",
    "- 對「正規化如何減少過擬合」建立一些直覺"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
