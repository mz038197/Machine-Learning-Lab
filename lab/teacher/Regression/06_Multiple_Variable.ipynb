{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7388c93",
   "metadata": {},
   "source": [
    "# **多變數線性迴歸（Multiple Variable Linear Regression）**\n",
    "\n",
    "在本實驗中，你會在先前單一特徵的基礎上，將資料結構與相關函式擴充為支援**多個特徵（features）**。\n",
    "\n",
    "# **1. 大綱**\n",
    "- [&nbsp;&nbsp;1.1 目標](#toc_15456_1.1)\n",
    "- [&nbsp;&nbsp;1.2 工具](#toc_15456_1.2)\n",
    "- [&nbsp;&nbsp;1.3 符號說明](#toc_15456_1.3)\n",
    "- [2 問題描述](#toc_15456_2)\n",
    "- [&nbsp;&nbsp;2.1 範例矩陣 X](#toc_15456_2.1)\n",
    "- [&nbsp;&nbsp;2.2 參數向量 w 與 b](#toc_15456_2.2)\n",
    "- [3 多變數模型的預測](#toc_15456_3)\n",
    "- [&nbsp;&nbsp;3.1 逐元素計算的單一預測](#toc_15456_3.1)\n",
    "- [&nbsp;&nbsp;3.2 使用向量運算的單一預測](#toc_15456_3.2)\n",
    "- [4 多變數情況下的成本函式](#toc_15456_4)\n",
    "- [5 多變數情況下的梯度下降](#toc_15456_5)\n",
    "- [&nbsp;&nbsp;5.1 多變數情況下的梯度計算](#toc_15456_5.1)\n",
    "- [&nbsp;&nbsp;5.2 多變數情況下的梯度下降實作](#toc_15456_5.2)\n",
    "- [6 恭喜](#toc_15456_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c64de50",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.1\"></a>\n",
    "## 1.1 目標\n",
    "\n",
    "- 將線性迴歸模型的程式擴充為支援多個特徵\n",
    "\n",
    "    - 擴充資料結構以支援多個特徵\n",
    "\n",
    "    - 重新撰寫預測、成本與梯度計算等函式，使其支援多個特徵\n",
    "\n",
    "    - 使用 NumPy 的 `np.dot` 進行向量化實作，以提升速度並簡化程式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb5f505",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.2\"></a>\n",
    "## 1.2 工具\n",
    "在本實驗中，我們會使用： \n",
    "- **NumPy**：常用的科學運算函式庫\n",
    "- **Matplotlib**：常用的資料視覺化與繪圖函式庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import copy, math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "#region 匯入相關模組\n",
    "\n",
    "def find_repo_root(marker=\"README.md\"):\n",
    "    cur = Path.cwd()\n",
    "    while cur != cur.parent:  # 防止無限迴圈，到達檔案系統根目錄就停\n",
    "        if (cur / marker).exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    return None\n",
    "\n",
    "def import_data_from_github():\n",
    "    import os, urllib.request, pathlib, shutil\n",
    "    \n",
    "    def isRunningInColab() -> bool:\n",
    "        return \"google.colab\" in sys.modules\n",
    "\n",
    "    def isRunningInJupyterLab() -> bool:\n",
    "        try:\n",
    "            import jupyterlab\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "        \n",
    "    def detect_env():\n",
    "        from IPython import get_ipython\n",
    "        if isRunningInColab():\n",
    "            return \"Colab\"\n",
    "        elif isRunningInJupyterLab():\n",
    "            return \"JupyterLab\"\n",
    "        elif \"notebook\" in str(type(get_ipython())).lower():\n",
    "            return \"Jupyter Notebook\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "    def get_utils_dir(env): \n",
    "        if env == \"Colab\": \n",
    "            if \"/content\" not in sys.path:\n",
    "                sys.path.insert(0, \"/content\")\n",
    "            return \"/content/utils\"\n",
    "        else:\n",
    "            return Path.cwd() / \"utils\"\n",
    "\n",
    "    env = detect_env()\n",
    "    UTILS_DIR = get_utils_dir(env)\n",
    "    REPO_DIR = \"Machine-Learning-Lab\"\n",
    "\n",
    "    os.makedirs(UTILS_DIR, exist_ok=True)\n",
    "\n",
    "    BASE = f\"https://raw.githubusercontent.com/mz038197/{REPO_DIR}/main\"\n",
    "    urllib.request.urlretrieve(f\"{BASE}/utils/deeplearning.mplstyle\", f\"{UTILS_DIR}/deeplearning.mplstyle\")\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "\n",
    "if repo_root is None:\n",
    "    import_data_from_github()\n",
    "    repo_root = Path.cwd()\n",
    "    \n",
    "os.chdir(repo_root)\n",
    "print(f\"✅ 切換工作目錄至 {Path.cwd()}\")\n",
    "sys.path.append(str(repo_root)) if str(repo_root) not in sys.path else None\n",
    "print(f\"✅ 加入到系統路徑\")\n",
    "\n",
    "plt.style.use('utils/deeplearning.mplstyle')\n",
    "print(\"✅ 匯入模組及設定繪圖樣式\")\n",
    "\n",
    "#endregion 匯入相關模組"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2437e",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.3\"></a>\n",
    "\n",
    "## 1.3 符號說明（Notation）\n",
    "下面整理了在多特徵情況下會出現的一些常用符號與對應的 Python 實作名稱：  \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| 一般符號 <img width=70/> <br />  Notation  <img width=70/> | 說明<img width=350/> | 對應的 Python 名稱（若適用） |\n",
    "| ------------| ------------------------------------------------------------|--|\n",
    "| $a$ | 純量（scalar），一般非粗體字                                                      | |\n",
    "| $\\mathbf{a}$ | 向量（vector），使用粗體小寫字                                                 | |\n",
    "| $\\mathbf{A}$ | 矩陣（matrix），使用粗體大寫字                                         | |\n",
    "| **迴歸問題（Regression）** |         |    |\n",
    "|  $\\mathbf{X}$ | 訓練樣本矩陣（每列一個樣本）                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | 訓練樣本的目標值向量                | `y_train` |\n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | 第 $i$ 筆訓練樣本與其標籤 | `X[i]`, `y[i]`|\n",
    "| m | 訓練樣本的數量 | `m`|\n",
    "| n | 每個樣本的特徵數量 | `n`|\n",
    "|  $\\mathbf{w}$  |  權重參數向量（weights）                       | `w`    |\n",
    "|  $b$           |  偏差（bias）參數                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | 以參數 $\\mathbf{w},b$ 在樣本 $\\mathbf{x^{(i)}}$ 上得到的模型輸出：$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae8ac7",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c167658",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2\"></a>\n",
    "\n",
    "# **2. 問題描述**\n",
    "\n",
    "我們會延續**房價預測**這個動機範例。訓練資料集中共有三筆樣本，每筆包含四個特徵（房屋坪數、臥室數、樓層數與屋齡），如下表所示。請注意：與先前實驗不同，這裡的房屋大小單位是「平方英尺（sqft）」，而不是「千平方英尺（1000 sqft）」。這將導致一些問題，我們會在下一個實驗中解決！\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n",
    "</div>\n",
    "\n",
    "你會根據這些資料建立一個線性迴歸模型，之後即可用它來預測其他房屋的價格，例如：一間 1200 平方英尺、3 間臥室、1 層樓、屋齡 40 年的房子。  \n",
    "\n",
    "請完成下方程式碼 cell 來建立 `X_train` 與 `y_train` 變數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aef09b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0afd598c92983b06",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c0686",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2.1\"></a>\n",
    "## 2.1 範例矩陣 X\n",
    "與上表類似，訓練樣本會存放在 NumPy 矩陣 `X_train` 中。矩陣的每一列代表一筆訓練樣本。當你有 $m$ 筆訓練樣本（在本例中 $m=3$），且每筆樣本有 $n$ 個特徵（本例中 $n=4$），則 $\\mathbf{X}$ 是一個維度為 $(m, n)$ 的矩陣（m 列、n 行）。\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "符號說明：\n",
    "- $\\mathbf{x}^{(i)}$ 是第 $i$ 筆樣本所對應的向量：$\\mathbf{x}^{(i)} = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
    "- $x^{(i)}_j$ 是第 $i$ 筆樣本中的第 $j$ 個特徵值。括號中的上標表示「第幾筆樣本」，而下標則代表「第幾個特徵」。  \n",
    "\n",
    "請先將輸入資料印出來觀察。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is stored in numpy array/matrix\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(X_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627ac27c",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2.2\"></a>\n",
    "## 2.2 參數向量 w 與 b\n",
    "\n",
    "* $\\mathbf{w}$ 是一個包含 $n$ 個元素的向量。\n",
    "  - 每個元素對應一個特徵的參數。\n",
    "  - 在本資料集中，$n = 4$。\n",
    "  - 在概念圖上，我們通常將 $\\mathbf{w}$ 畫成一個「欄向量（column vector）」：\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $b$ 則是單一的純量參數（bias）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f17d5",
   "metadata": {},
   "source": [
    "在示範中，我們會先將 $\\mathbf{w}$ 與 $b$ 設定為一組「接近最佳解」的初始值。這裡的 $\\mathbf{w}$ 是一個一維的 NumPy 向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18359fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fc3462",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce50c6",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3\"></a>\n",
    "\n",
    "# **3. 多變數模型的預測**\n",
    "\n",
    "在多變數線性迴歸中，模型的預測可以寫成下列線性形式：\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "或使用向量記號：\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "其中 $\\cdot$ 表示向量的「內積（dot product）」。\n",
    "\n",
    "為了示範內積的用法，我們會分別依照式 (1) 與式 (2) 來實作預測函式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b667fce",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3.1\"></a>\n",
    "## 3.1 逐元素計算的單一預測\n",
    "在前面的單變數線性迴歸中，我們的預測方式是：將一個特徵值乘上一個參數，最後再加上偏差參數（bias）。\n",
    "\n",
    "若要直接把這個作法擴充到多個特徵，一個直覺的作法就是根據上面的式 (1)，對每一個特徵使用迴圈：逐一將「特徵值 × 對應參數」加總起來，最後再加上偏差參數 $b$。\n",
    "\n",
    "請用迴圈型式完成函式 `predict_single_loop(x, w, b)` 於下方cell中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfbbb5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4529039bb2a60a12",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_single_loop(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters    \n",
    "      b (scalar):  model parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    p = 0\n",
    "    for i in range(n):\n",
    "        p_i = x[i] * w[i]  \n",
    "        p = p + p_i         \n",
    "    p = p + b                \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec9d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31254e55",
   "metadata": {},
   "source": [
    "請留意 `x_vec` 的形狀：它是一個具有 4 個元素的一維 NumPy 向量，shape 為 `(4,)`。而 `f_wb` 則是一個純量（scalar）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ae357",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3.2\"></a>\n",
    "## 3.2 使用向量運算的單一預測\n",
    "\n",
    "注意到，上面的式 (1) 可以改寫為式 (2) 所示的向量內積形式，因此我們可以運用向量運算，來加速預測的計算。\n",
    "\n",
    "回想在 Python/NumPy 實驗中提到的，NumPy 的 `np.dot()` [[官方文件](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)] 可以用來計算向量內積。 \n",
    "\n",
    "請用向量運算型式來完成函式 `predict(x, w, b)` 與下方cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b3ef5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f410123d762567fb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict(x_vec,w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec759d",
   "metadata": {},
   "source": [
    "可以看到，這裡得到的結果與使用迴圈時完全相同，shape 也一致。\n",
    "\n",
    "接下來在本課程中，這類運算會盡量改用 `np.dot` 完成。這樣預測就只需要一行程式碼，因此在後續的函式中，多半會直接嵌入這個運算，而不額外呼叫獨立的 `predict` 函式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4a810",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210f976",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_4\"></a>\n",
    "\n",
    "# **4. 多變數情況下的成本函式**\n",
    "\n",
    "在多變數線性迴歸中，成本函式 $J(\\mathbf{w},b)$ 可寫為：\n",
    "\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "\n",
    "其中：\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "與之前單一特徵的實驗不同的是，這裡的 $\\mathbf{w}$ 與 $\\mathbf{x}^{(i)}$ 都是**向量**，用來同時處理多個特徵，而不再是單一純量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31631b4c",
   "metadata": {},
   "source": [
    "請完成下方函式 `compute_cost(X, y, w, b)` 程式碼，來實作上面 式 (3) 與式 (4)。\n",
    "\n",
    "注意：請使用一個 for 迴圈，遍歷所有 `m` 筆訓練樣本，逐筆累加成本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9ead6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58de716bc11383c9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
    "    cost = cost / (2 * m)                      #scalar    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display cost using our pre-chosen optimal parameters. \n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea0c775",
   "metadata": {},
   "source": [
    "**預期結果**：Cost at optimal w : 1.5578904045996674e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba434c5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63399edc",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "\n",
    "# **5. 多變數情況下的梯度下降**\n",
    "\n",
    "多變數線性迴歸的梯度下降可寫成：\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "其中，$n$ 是特徵數量，各個參數 $w_j$ 與 $b$ 會在每一步同時更新，而偏導數為：  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* $m$：資料集中訓練樣本的數量\n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$：模型在第 $i$ 筆樣本上的預測值，而 $y^{(i)}$ 是對應的目標值（真實標籤）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce39e8",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5.1\"></a>\n",
    "## 5.1 多變數情況下的梯度計算\n",
    "\n",
    "請完成下方函式 `compute_gradient(X, y, w, b)` 程式碼，來實作上面 式 (6) 與式 (7) 的梯度計算。\n",
    "\n",
    "其實有很多種寫法，這邊我們限定採用：\n",
    "\n",
    "- 外層迴圈：遍歷所有 $m$ 筆樣本\n",
    "\n",
    "    - 對每一筆樣本，直接計算並累加對 $b$ 的偏導數 $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$\n",
    "\n",
    "    - 再在一個內層迴圈中，針對所有 $n$ 個特徵：\n",
    "        - 計算並累加每個 $w_j$ 對應的偏導數 $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f7057",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-49d6a9312840cece",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i] #scalar\n",
    "        dj_dw = dj_dw + err * X[i]\n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3deac5c",
   "metadata": {},
   "source": [
    "下方函式 `compute_gradient_vectorization(X, y, w, b)` 程式碼為全向量化實作上面 式 (6) 與式 (7) 的梯度計算。(供參考)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ecfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_vectorization(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    y_pred = np.dot(X, w) + b      # (m,)\n",
    "    error = y_pred - y             # (m,)\n",
    "    \n",
    "    dj_dw = (1/m) * np.dot(X.T, error)  # (n,)\n",
    "    dj_db = (1/m) * np.sum(error)       # scalar\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22af3da5",
   "metadata": {},
   "source": [
    "**預期結果**：   \n",
    " dj_db at initial w,b: -1.6739251122999121e-06  \n",
    " dj_dw at initial w,b:   \n",
    " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fb2d4",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5.2\"></a>\n",
    "## 5.2 多變數情況下的梯度下降實作\n",
    "下面這個函式實作了上面式 (5) 所描述的梯度下降過程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3804c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d92ec",
   "metadata": {},
   "source": [
    "在下一個程式碼 cell 中，你會實際測試這個梯度下降實作。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0edee3",
   "metadata": {},
   "source": [
    "**預期結果**：    \n",
    " b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07]   \n",
    " prediction: 426.19, target value: 460  \n",
    " prediction: 286.17, target value: 232  \n",
    " prediction: 171.47, target value: 178    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8f9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_hist)\n",
    "ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69d1c6",
   "metadata": {},
   "source": [
    "*這樣的結果並不是特別令人振奮*！成本仍然在持續下降，而且目前的預測也還不夠準確。下一個實驗會探討如何改進這種情況。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96ffe2a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e6b57",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"toc_15456_6\"></a>\n",
    "\n",
    "# **6. 恭喜！**\n",
    "\n",
    "在本實驗中，你已經：\n",
    "- 重新整理並擴充了線性迴歸所需的各種函式，讓它們可以處理多個特徵。\n",
    "- 使用 NumPy 的 `np.dot` 來將實作向量化，以提升運算效率與簡潔度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698888ca",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "15456"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
