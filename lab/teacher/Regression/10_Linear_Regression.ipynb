{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習測驗：線性迴歸（Linear Regression）\n",
    "\n",
    "歡迎來到第一個測驗實驗！在這個實驗中，你將要實作「單變數線性迴歸」，用來預測連鎖餐廳在不同城市的獲利。\n",
    "\n",
    "\n",
    "## 大綱\n",
    "- [ 1 - 套件（Packages）](#1)\n",
    "- [ 2 - 單變數線性迴歸 ](#2)\n",
    "  - [ 2.1 問題說明（Problem Statement）](#2.1)\n",
    "  - [ 2.2 資料集（Dataset）](#2.2)\n",
    "  - [ 2.3 線性迴歸複習（Refresher on linear regression）](#2.3)\n",
    "  - [ 2.4 成本函數計算（Compute Cost）](#2.4)\n",
    "    - [ 練習 1（Exercise 1）](#ex01)\n",
    "  - [ 2.5 梯度下降法（Gradient descent）](#2.5)\n",
    "    - [ 練習 2（Exercise 2）](#ex02)\n",
    "  - [ 2.6 使用批次梯度下降學習參數（Learning parameters using batch gradient descent）](#2.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**注意：** 為了避免自動評分系統（autograder）出錯，**請不要**編輯或刪除這份 notebook 中「非評分」的程式碼區塊，也請**不要新增**任何新的程式碼儲存格。\n",
    "**在你順利通過本次作業之後**，如果想對非評分的程式碼進行實驗，可以參考本 notebook 最底部的說明進行調整。_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - 套件（Packages）\n",
    "\n",
    "首先，請執行下方的程式碼儲存格，匯入在本次作業中會用到的所有套件：\n",
    "- [numpy](https://www.numpy.org)：在 Python 中處理矩陣與向量運算的核心套件。\n",
    "- [matplotlib](https://matplotlib.org)：常用的繪圖函式庫，用來畫各種圖表。\n",
    "- `utils.py`：本作業會用到的一些輔助函式都放在這個檔案中，你**不需要**修改裡面的任何程式碼。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 切換工作目錄至 d:\\Work\\Python\\Machine-Learning-Lab\n",
      "✅ 加入到系統路徑\n",
      "✅ 匯入模組及設定繪圖樣式\n"
     ]
    }
   ],
   "source": [
    "# region 資料載入\n",
    "import copy\n",
    "import math\n",
    "import sys, os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def find_repo_root(marker=\"README.md\"):\n",
    "    cur = Path.cwd()\n",
    "    while cur != cur.parent:  # 防止無限迴圈，到達檔案系統根目錄就停\n",
    "        if (cur / marker).exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    return None\n",
    "\n",
    "def import_data_from_github():\n",
    "    import urllib.request, shutil\n",
    "    \n",
    "    def isRunningInColab() -> bool:\n",
    "        return \"google.colab\" in sys.modules\n",
    "\n",
    "    def isRunningInJupyterLab() -> bool:\n",
    "        try:\n",
    "            import jupyterlab\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "        \n",
    "    def detect_env():\n",
    "        from IPython import get_ipython\n",
    "        if isRunningInColab():\n",
    "            return \"Colab\"\n",
    "        elif isRunningInJupyterLab():\n",
    "            return \"JupyterLab\"\n",
    "        elif \"notebook\" in str(type(get_ipython())).lower():\n",
    "            return \"Jupyter Notebook\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "    def get_utils_dir(env): \n",
    "        if env == \"Colab\": \n",
    "            if \"/content\" not in sys.path:\n",
    "                sys.path.insert(0, \"/content\")\n",
    "            return \"/content/utils\"\n",
    "        else:\n",
    "            return Path.cwd() / \"utils\"\n",
    "\n",
    "    def get_data_dir(env): \n",
    "        if env == \"Colab\": \n",
    "            if \"/content\" not in sys.path:\n",
    "                sys.path.insert(0, \"/content\")\n",
    "            return \"/content/data\"\n",
    "        else:\n",
    "            return Path.cwd() / \"data\"\n",
    "\n",
    "    def get_images_dir(env): \n",
    "        if env == \"Colab\": \n",
    "            if \"/content\" not in sys.path:\n",
    "                sys.path.insert(0, \"/content\")\n",
    "            return f\"/content/images\"\n",
    "        else:\n",
    "            return Path.cwd() / \"images\"\n",
    "\n",
    "    env = detect_env()\n",
    "    UTILS_DIR = get_utils_dir(env)\n",
    "    DATA_DIR = get_data_dir(env)\n",
    "    IMG_DIR = get_images_dir(env)\n",
    "\n",
    "    REPO_DIR = \"Machine-Learning-Lab\"\n",
    "\n",
    "    os.makedirs(UTILS_DIR, exist_ok=True)\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "    BASE = f\"https://raw.githubusercontent.com/mz038197/{REPO_DIR}/main\"\n",
    "\n",
    "    utils_list = [\"utils.py\", \"public_tests.py\"]\n",
    "    for u in utils_list:\n",
    "        urllib.request.urlretrieve(f\"{BASE}/utils/{u}\", f\"{UTILS_DIR}/{u}\")\n",
    "\n",
    "    data_list = [\"ex1data1.txt\", \"ex1data2.txt\"]\n",
    "    for d in data_list:\n",
    "        urllib.request.urlretrieve(f\"{BASE}/data/{d}\", f\"{DATA_DIR}/{d}\")\n",
    "\n",
    "    # images_list = [\"shortRun.PNG\", \"longRun.PNG\", \"scale.PNG\"]\n",
    "    # for image in images_list:\n",
    "    #     urllib.request.urlretrieve(f\"{BASE}/lab/teacher/Regression/images/{image}\", f\"{IMG_DIR}/{image}\")\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "\n",
    "if repo_root is None:\n",
    "    import_data_from_github()\n",
    "    repo_root = Path.cwd()\n",
    "    \n",
    "os.chdir(repo_root)\n",
    "print(f\"✅ 切換工作目錄至 {Path.cwd()}\")\n",
    "sys.path.append(str(repo_root)) if str(repo_root) not in sys.path else None\n",
    "print(f\"✅ 加入到系統路徑\")\n",
    "\n",
    "from utils.utils import *\n",
    "\n",
    "plt.style.use('utils/deeplearning.mplstyle')\n",
    "print(\"✅ 匯入模組及設定繪圖樣式\")\n",
    "#endregion 資料載入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 問題說明（Problem Statement）\n",
    "\n",
    "假設你是一間連鎖餐廳的執行長，正在評估要在哪些城市開設新的分店。\n",
    "- 你希望將事業擴展到**能帶來較高獲利**的城市。\n",
    "- 這個連鎖品牌已經在許多城市設有餐廳，並且蒐集了各城市的人口與餐廳獲利資料。\n",
    "- 此外，你也擁有一些**潛在新據點城市**的資料：\n",
    "    - 對這些城市，你掌握的是城市人口數。\n",
    "    \n",
    "你能否利用這些資料，幫助你判斷**哪些城市比較可能帶來較高的餐廳獲利**？\n",
    "\n",
    "## 3 - 資料集（Dataset）\n",
    "\n",
    "你將先從載入本次任務所需的資料開始。\n",
    "- 下方的 `load_data()` 函式會把資料讀入變數 `x_train` 和 `y_train`：\n",
    "  - `x_train`：各城市的人口數\n",
    "  - `y_train`：該城市中餐廳的獲利（若為負值表示虧損）  \n",
    "  - `x_train` 與 `y_train` 都是 numpy 的陣列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "x_train, y_train = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 檢視變數內容\n",
    "在開始實作之前，先熟悉一下資料集是很有幫助的。  \n",
    "- 一個簡單的方式，就是把每個變數印出來，看它的型態與前幾筆資料。\n",
    "\n",
    "下面的程式碼會印出變數 `x_train`，以及它的型態。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of x_train: <class 'numpy.ndarray'>\n",
      "First five elements of x_train are:\n",
      " [6.1101 5.5277 8.5186 7.0032 5.8598]\n"
     ]
    }
   ],
   "source": [
    "# print x_train\n",
    "print(\"Type of x_train:\",type(x_train))\n",
    "print(\"First five elements of x_train are:\\n\", x_train[:5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_train` 是一個 numpy 陣列，裡面都是大於 0 的小數。 \n",
    "- 這些數值代表「城市人口數／10,000」。\n",
    "- 例如：6.1101 代表該城市人口為 61,101 人。\n",
    "  \n",
    "接下來，我們來印出 `y_train`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of y_train: <class 'numpy.ndarray'>\n",
      "First five elements of y_train are:\n",
      " [17.592   9.1302 13.662  11.854   6.8233]\n"
     ]
    }
   ],
   "source": [
    "# print y_train\n",
    "print(\"Type of y_train:\",type(y_train))\n",
    "print(\"First five elements of y_train are:\\n\", y_train[:5])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同樣地，`y_train` 也是一個 numpy 陣列，包含正值與負值的小數。\n",
    "- 這些數值代表該城市餐廳的「平均每月獲利」，單位是 \\$10,000（也就是一萬美元）。\n",
    "  - 例如：17.592 代表該城市平均每月獲利 \\$175,920。\n",
    "  - -2.6807 則代表該城市平均每月虧損 \\$26,807。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 檢查變數的維度（dimensions）\n",
    "\n",
    "另一個熟悉資料的好方法，是觀察各變數的「維度」（shape）。\n",
    "\n",
    "請印出 `x_train` 與 `y_train` 的 shape，看看這個資料集中共有多少筆訓練樣本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print ('The shape of x_train is:', x_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "城市人口的陣列共有 97 筆資料，月平均獲利的陣列也有 97 筆資料。這兩個變數都是 NumPy 的一維（1D）陣列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 視覺化你的資料\n",
    "\n",
    "用圖像來呈現資料，常常有助於理解其結構與關係。\n",
    "- 對這個資料集來說，只包含兩個變數（獲利與人口），因此可以用散佈圖（scatter plot）來視覺化。\n",
    "- 在真實世界的問題中，通常會有兩個以上的特徵（例如：人口、家庭平均收入、每月獲利、每月銷售額等）。就算有多個特徵，你仍可以用散佈圖來觀察「任兩個特徵」之間的關係。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a scatter plot of the data. To change the markers to red \"x\",\n",
    "# we used the 'marker' and 'c' parameters\n",
    "plt.scatter(x_train, y_train, marker='x', c='r') \n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Profits vs. Population per city\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Profit in $10,000')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你的目標是建立一個線性迴歸模型，使其能夠良好地擬合這筆資料。\n",
    "- 有了這個模型之後，只要輸入某個新城市的人口，就可以估計在該城市開設餐廳時的「每月獲利」可能是多少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - 線性迴歸複習（Refresher on linear regression）\n",
    "\n",
    "在這個練習實驗中，你會為資料集擬合線性迴歸模型的參數 $(w,b)$。\n",
    "- 單變數線性迴歸的模型函數會把輸入 `x`（城市人口）映射到輸出 `y`（該城市餐廳的每月獲利），可以寫成：\n",
    "    $$f_{w,b}(x) = wx + b$$\n",
    "    \n",
    "\n",
    "- 要訓練一個線性迴歸模型，就是要找到一組最適合資料的參數 $(w,b)$。\n",
    "\n",
    "    - 為了比較不同 $(w,b)$ 的好壞，我們會定義一個成本函數（cost function）$J(w,b)$。\n",
    "      - $J$ 是 $(w,b)$ 的函數，也就是說，成本 $J(w,b)$ 的數值取決於你選擇的參數 $(w,b)$。\n",
    "  \n",
    "    - 那組「最適合」資料的 $(w,b)$，就是讓成本函數 $J(w,b)$ 最小的那一組參數。\n",
    "\n",
    "\n",
    "- 為了找出能讓成本 $J(w,b)$ 最小的參數 $(w,b)$，我們會使用一種方法：**梯度下降（gradient descent）**。\n",
    "  - 梯度下降每走一步，就會更新一次 $(w,b)$，讓它們越來越接近讓成本函數最小的最佳值。\n",
    "  \n",
    "\n",
    "- 訓練完成後的線性迴歸模型，可以接收輸入特徵 $x$（城市人口），並輸出對應的預測值 $f_{w,b}(x)$（預測在該城市的餐廳每月獲利）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - 成本函數計算（Compute Cost）\n",
    "\n",
    "梯度下降法會重複地更新參數 $(w,b)$，讓成本函數 $J(w,b)$ 逐步變小。\n",
    "- 在每一步更新 $(w,b)$ 之後，如果能計算當前的成本 $J(w,b)$，就能觀察訓練是否朝正確方向進行。\n",
    "- 在本節中，你會實作一個函式來計算 $J(w,b)$，方便你檢查梯度下降演算法的執行情況。\n",
    "\n",
    "#### 成本函數（Cost function）\n",
    "你在課程中應該已經看過，對單一變數線性迴歸而言，成本函數 $J(w,b)$ 定義如下：\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$ \n",
    "\n",
    "- 你可以把 $f_{w,b}(x^{(i)})$ 想成：模型對第 $i$ 筆資料的「預測獲利」，而 $y^{(i)}$ 則是資料中實際記錄的「真實獲利」。\n",
    "- 其中 $m$ 是資料集中訓練樣本的數量。\n",
    "\n",
    "#### 模型預測（Model prediction）\n",
    "\n",
    "- 對於單變數線性迴歸，第 $i$ 筆樣本的模型預測值 $f_{w,b}(x^{(i)})$ 可以寫成：\n",
    "\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b$$\n",
    "\n",
    "這就是一條直線的方程式，其中 $b$ 是截距（intercept），$w$ 是斜率（slope）。\n",
    "\n",
    "#### 實作說明（Implementation）\n",
    "\n",
    "請完成下方的 `compute_cost()` 函式，計算成本函數 $J(w,b)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex01\"></a>\n",
    "### 練習 1（Exercise 1）\n",
    "\n",
    "請完成下面的 `compute_cost` 函式，讓它能夠：\n",
    "\n",
    "* 針對每一筆訓練資料進行迴圈，並且計算：\n",
    "    * 該筆資料的模型預測值：\n",
    "    $$\n",
    "    f_{wb}(x^{(i)}) =  wx^{(i)} + b \n",
    "    $$\n",
    "   \n",
    "    * 該筆資料對應的成本：\n",
    "    $$cost^{(i)} =  (f_{wb} - y^{(i)})^2$$\n",
    "    \n",
    "\n",
    "* 回傳所有樣本的總成本：\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} cost^{(i)}$$\n",
    "  * 其中，$m$ 為訓練樣本數量，$\\sum$ 則為求和符號。\n",
    "\n",
    "如果卡住了，你可以參考此儲存格後面提供的提示來完成實作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Shape (m,) Input to the model (Population of cities) \n",
    "        y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
    "        w, b (scalar): Parameters of the model\n",
    "    \n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = x.shape[0] \n",
    "    \n",
    "    # You need to return this variable correctly\n",
    "    total_cost = 0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>點此展開提示</b></font></summary>\n",
    "    \n",
    "   * 你可以用下列程式碼來表示一個求和運算，例如：$h = \\sum\\limits_{i = 0}^{m-1} 2i$\n",
    "    \n",
    "    ```python \n",
    "    h = 0\n",
    "    for i in range(m):\n",
    "        h = h + 2*i\n",
    "    ```\n",
    "  \n",
    "   * 在這個問題中，你可以用 `for` 迴圈走過 `x` 中的每一筆資料，並在每次迭代中把對應的 `cost` 加到一個在迴圈外初始化的變數 `cost_sum` 中。\n",
    "\n",
    "   * 最後，將 `cost_sum` 除以 `2m`，就可以得到 `total_cost`。\n",
    "   * 如果你是 Python 新手，請確認你的縮排（空白或 tab）是一致的；否則可能會得到錯誤的結果，或出現 `IndentationError: unexpected indent`。你可以參考社群中的這篇說明：[這個主題](https://community.deeplearning.ai/t/indentation-in-python-indentationerror-unexpected-indent/159398)。\n",
    "\n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> 點此獲得更多提示</b></font></summary>\n",
    "        \n",
    "    * 以下是這個函式的一種實作架構：\n",
    "    \n",
    "    ```python \n",
    "    def compute_cost(x, y, w, b):\n",
    "        # 訓練樣本數\n",
    "        m = x.shape[0] \n",
    "    \n",
    "        # 需正確回傳的變數\n",
    "        total_cost = 0\n",
    "    \n",
    "        ### START CODE HERE ###  \n",
    "        # 用來累加每個樣本成本的變數\n",
    "        cost_sum = 0\n",
    "    \n",
    "        # 走過每一筆訓練資料\n",
    "        for i in range(m):\n",
    "            # 這裡填入第 i 筆資料的預測值 f_wb\n",
    "            f_wb = \n",
    "            # 這裡填入第 i 筆資料的成本 cost\n",
    "            cost = \n",
    "        \n",
    "            # 累加成本\n",
    "            cost_sum = cost_sum + cost \n",
    "\n",
    "        # 將總和除以 (2*m) 得到最終成本\n",
    "        total_cost = (1 / (2 * m)) * cost_sum\n",
    "        ### END CODE HERE ### \n",
    "\n",
    "        return total_cost\n",
    "    ```\n",
    "    \n",
    "    * 如果還是卡住，你可以再看下面關於如何計算 `f_wb` 與 `cost` 的提示。\n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>關於如何計算 f_wb 的提示</b></font></summary>\n",
    "           &emsp; &emsp; 對於純量 $a$、$b$、$c$（這裡的 <code>x[i]</code>、<code>w</code>、<code>b</code> 都是純量），若要計算 $h = ab + c$，可以寫成 <code>h = a * b + c</code>\n",
    "          <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; 更多關於 f 的提示</b></font></summary>\n",
    "               &emsp; &emsp; 你可以這樣計算 f_wb：<code>f_wb = w * x[i] + b </code>\n",
    "           </details>\n",
    "    </details>\n",
    "\n",
    "     <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>關於如何計算 cost 的提示</b></font></summary>\n",
    "          &emsp; &emsp; 若要計算變數 z 的平方，可以寫成 <code>z**2</code>\n",
    "          <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; 更多關於 cost 的提示</b></font></summary>\n",
    "              &emsp; &emsp; 你可以這樣計算 cost：<code>cost = (f_wb - y[i]) ** 2</code>\n",
    "          </details>\n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "\n",
    "</details>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以執行下面的測試程式碼，檢查自己實作的結果是否正確："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Compute cost with some initial values for paramaters w, b\n",
    "initial_w = 2\n",
    "initial_b = 1\n",
    "\n",
    "cost = compute_cost(x_train, y_train, initial_w, initial_b)\n",
    "print(type(cost))\n",
    "print(f'Cost at initial w: {cost:.3f}')\n",
    "\n",
    "# Public tests\n",
    "from utils.public_tests import *\n",
    "compute_cost_test(compute_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**預期輸出（Expected Output）**：\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Cost at initial w:<b> 75.203 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - 梯度下降法（Gradient descent） \n",
    "\n",
    "在本節中，你將為線性迴歸實作參數 $w, b$ 的梯度計算函式。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在課程影片中，我們介紹了梯度下降演算法，其形式如下：\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\phantom {0000} b := b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\newline       \\; & \\phantom {0000} w := w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{1}  \\; & \n",
    "\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "在這裡，參數 $w, b$ 會同時更新，其中  \n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)} \\tag{3}\n",
    "$$\n",
    "* 其中，$m$ 是訓練樣本的數量。\n",
    "\n",
    "    \n",
    "*  $f_{w,b}(x^{(i)})$ 是模型對第 $i$ 筆資料的預測值，而 $y^{(i)}$ 則是該筆資料的真實值。\n",
    "\n",
    "\n",
    "接下來，你將實作一個名為 `compute_gradient` 的函式，用來計算 $\\frac{\\partial J(w,b)}{\\partial w}$ 與 $\\frac{\\partial J(w,b)}{\\partial b}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex02\"></a>\n",
    "### 練習 2（Exercise 2）\n",
    "\n",
    "請完成 `compute_gradient` 函式，讓它能夠：\n",
    "\n",
    "* 對每一筆訓練資料進行迴圈，並在每筆資料上計算：\n",
    "    * 該筆資料的模型預測值：\n",
    "    $$\n",
    "    f_{wb}(x^{(i)}) =  wx^{(i)} + b \n",
    "    $$\n",
    "   \n",
    "    * 該筆資料對應的梯度：\n",
    "        $$\n",
    "        \\frac{\\partial J(w,b)}{\\partial b}^{(i)}  =  (f_{w,b}(x^{(i)}) - y^{(i)}) \n",
    "        $$\n",
    "        $$\n",
    "        \\frac{\\partial J(w,b)}{\\partial w}^{(i)}  =  (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)} \n",
    "        $$\n",
    "    \n",
    "\n",
    "* 回傳對所有樣本求和後的總梯度：\n",
    "    $$\n",
    "    \\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{\\partial J(w,b)}{\\partial b}^{(i)}\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    \\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{\\partial J(w,b)}{\\partial w}^{(i)} \n",
    "    $$\n",
    "  * 其中，$m$ 是訓練樣本的數量，$\\sum$ 表示加總運算。\n",
    "\n",
    "如果遇到困難，你可以參考此儲存格後方的提示來協助完成實作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_gradient\n",
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) Input to the model (Population of cities) \n",
    "      y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
    "      w, b (scalar): Parameters of the model  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>點此展開提示</b></font></summary>\n",
    "    \n",
    "   * 你可以用下列程式碼來表示一個求和運算，例如：$h = \\sum\\limits_{i = 0}^{m-1} 2i$\n",
    "    \n",
    "   ```python \n",
    "    h = 0\n",
    "    for i in range(m):\n",
    "        h = h + 2*i\n",
    "   ```\n",
    "    \n",
    "   * 在這個問題中，你可以用 `for` 迴圈走過 `x` 中的每一筆資料，並在每次迭代中把該筆資料貢獻的梯度加入到在迴圈外初始化的變數 `dj_dw` 和 `dj_db` 中。\n",
    "\n",
    "   * 最後，將 `dj_dw` 與 `dj_db` 都除以 `m`，就可以得到平均梯度。    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> 點此獲得更多提示</b></font></summary>\n",
    "        \n",
    "    * 以下是這個函式的一種實作架構：\n",
    "    \n",
    "    ```python \n",
    "    def compute_gradient(x, y, w, b): \n",
    "        \"\"\"\n",
    "        Computes the gradient for linear regression \n",
    "        Args:\n",
    "          x (ndarray): Shape (m,) Input to the model (Population of cities) \n",
    "          y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
    "          w, b (scalar): Parameters of the model  \n",
    "        Returns\n",
    "          dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "          dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "        \"\"\"\n",
    "    \n",
    "        # Number of training examples\n",
    "        m = x.shape[0]\n",
    "    \n",
    "        # You need to return the following variables correctly\n",
    "        dj_dw = 0\n",
    "        dj_db = 0\n",
    "    \n",
    "        ### START CODE HERE ### \n",
    "        # Loop over examples\n",
    "        for i in range(m):  \n",
    "            # Your code here to get prediction f_wb for the ith example\n",
    "            f_wb = \n",
    "            \n",
    "            # Your code here to get the gradient for w from the ith example \n",
    "            dj_dw_i = \n",
    "        \n",
    "            # Your code here to get the gradient for b from the ith example \n",
    "            dj_db_i = \n",
    "     \n",
    "            # Update dj_db : In Python, a += 1  is the same as a = a + 1\n",
    "            dj_db += dj_db_i\n",
    "        \n",
    "            # Update dj_dw\n",
    "            dj_dw += dj_dw_i\n",
    "    \n",
    "        # Divide both dj_dw and dj_db by m\n",
    "        dj_dw = dj_dw / m\n",
    "        dj_db = dj_db / m\n",
    "        ### END CODE HERE ### \n",
    "        \n",
    "        return dj_dw, dj_db\n",
    "    ```\n",
    "        \n",
    "    * If you're still stuck, you can check the hints presented below to figure out how to calculate `f_wb` and `cost`.\n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate f_wb</b></font></summary>\n",
    "           &emsp; &emsp; You did this in the previous exercise! For scalars $a$, $b$ and $c$ (<code>x[i]</code>, <code>w</code> and <code>b</code> are all scalars), you can calculate the equation $h = ab + c$ in code as <code>h = a * b + c</code>\n",
    "          <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; More hints to calculate f</b></font></summary>\n",
    "               &emsp; &emsp; You can compute f_wb as <code>f_wb = w * x[i] + b </code>\n",
    "           </details>\n",
    "    </details>\n",
    "        \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate dj_dw_i</b></font></summary>\n",
    "           &emsp; &emsp; For scalars $a$, $b$ and $c$ (<code>f_wb</code>, <code>y[i]</code> and <code>x[i]</code> are all scalars), you can calculate the equation $h = (a - b)c$ in code as <code>h = (a-b)*c</code>\n",
    "          <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; More hints to calculate f</b></font></summary>\n",
    "               &emsp; &emsp; You can compute dj_dw_i as <code>dj_dw_i = (f_wb - y[i]) * x[i] </code>\n",
    "           </details>\n",
    "    </details>\n",
    "        \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate dj_db_i</b></font></summary>\n",
    "             &emsp; &emsp; You can compute dj_db_i as <code> dj_db_i = f_wb - y[i] </code>\n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "\n",
    "</details>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請執行下面幾個程式儲存格，用兩組不同的初始參數 $w, b$ 來檢查你實作的 `compute_gradient` 函式是否正確。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Compute and display gradient with w initialized to zeroes\n",
    "initial_w = 0\n",
    "initial_b = 0\n",
    "\n",
    "tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, initial_w, initial_b)\n",
    "print('Gradient at initial w, b (zeros):', tmp_dj_dw, tmp_dj_db)\n",
    "\n",
    "compute_gradient_test(compute_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，我們來看看上述梯度計算在資料集上的表現。\n",
    "\n",
    "**預期輸出（Expected Output）**：\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Gradient at initial , b (zeros)<b></td>\n",
    "    <td> -65.32884975 -5.83913505154639</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Compute and display cost and gradient with non-zero w\n",
    "test_w = 0.2\n",
    "test_b = 0.2\n",
    "tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, test_w, test_b)\n",
    "\n",
    "print('Gradient at test w, b:', tmp_dj_dw, tmp_dj_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**預期輸出（Expected Output）**：\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Gradient at test w<b></td>\n",
    "    <td> -47.41610118 -4.007175051546391</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.6\"></a>\n",
    "### 2.6 使用批次梯度下降學習參數 \n",
    "\n",
    "接下來，你將利用「批次梯度下降」（batch gradient descent）來尋找線性迴歸模型的最適參數。\n",
    "- 這裡的 batch 指的是：**每次更新參數時，都使用所有訓練樣本**。\n",
    "- 這一節你不需要自己寫程式，只要執行下面幾個儲存格即可。\n",
    "\n",
    "- 檢查梯度下降是否運作正常的一個好方法，是觀察每一步的成本值 $J(w,b)$，看看它是否在持續下降。\n",
    "\n",
    "- 在你正確實作梯度與成本函數，並且選擇合適的學習率（learning rate） $\\alpha$ 的情況下，$J(w,b)$ 不應該上升，且會在演算法結束前逐漸收斂到一個穩定的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x :    (ndarray): Shape (m,)\n",
    "      y :    (ndarray): Shape (m,)\n",
    "      w_in, b_in : (scalar) Initial values of parameters of the model\n",
    "      cost_function: function to compute cost\n",
    "      gradient_function: function to compute the gradient\n",
    "      alpha : (float) Learning rate\n",
    "      num_iters : (int) number of iterations to run gradient descent\n",
    "    Returns\n",
    "      w : (ndarray): Shape (1,) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(x)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration — primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b )  \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(x, y, w, b)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            w_history.append(w)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the gradient descent algorithm above to learn the parameters for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# initialize fitting parameters. Recall that the shape of w is (n,)\n",
    "initial_w = 0.\n",
    "initial_b = 0.\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "\n",
    "w,b,_,_ = gradient_descent(x_train ,y_train, initial_w, initial_b, \n",
    "                     compute_cost, compute_gradient, alpha, iterations)\n",
    "print(\"w,b found by gradient descent:\", w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b> w, b found by gradient descent<b></td>\n",
    "    <td> 1.16636235 -3.63029143940436</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在我們將利用梯度下降得到的最終參數來繪製線性擬合直線。\n",
    "\n",
    "回顧一下，對於單一一筆樣本，其預測值可以寫成 $f(x^{(i)})= wx^{(i)}+b$。\n",
    "\n",
    "若要計算整個資料集中所有樣本的預測值，我們可以對所有訓練樣本進行迴圈，逐一計算每筆資料的預測結果，下方程式碼示範了這個做法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "m = x_train.shape[0]\n",
    "predicted = np.zeros(m)\n",
    "\n",
    "for i in range(m):\n",
    "    predicted[i] = w * x_train[i] + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，我們會把這些預測值畫出來，與原始資料點一起顯示，以觀察線性擬合的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Plot the linear fit\n",
    "plt.plot(x_train, predicted, c = \"b\")\n",
    "\n",
    "# Create a scatter plot of the data. \n",
    "plt.scatter(x_train, y_train, marker='x', c='r') \n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Profits vs. Population per city\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Profit in $10,000')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('Population of City in 10,000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你得到的最終參數 $w,b$ 也可以用來對新的城市進行獲利預測。下面我們估計人口為 35,000 與 70,000 的地區，其預期獲利為多少。\n",
    "\n",
    "- 這個模型的輸入是「城市人口（單位：10,000 人）」。\n",
    "\n",
    "- 因此，35,000 人可轉換為輸入 `np.array([3.5])`。\n",
    "\n",
    "- 同樣地，70,000 人可轉換為輸入 `np.array([7.])`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "predict1 = 3.5 * w + b\n",
    "print('For population = 35,000, we predict a profit of $%.2f' % (predict1*10000))\n",
    "\n",
    "predict2 = 7.0 * w + b\n",
    "print('For population = 70,000, we predict a profit of $%.2f' % (predict2*10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**預期輸出（Expected Output）**：\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b> For population = 35,000, we predict a profit of<b></td>\n",
    "    <td> $4519.77 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> <b> For population = 70,000, we predict a profit of<b></td>\n",
    "    <td> $45342.45 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**恭喜你完成這個線性迴歸的練習實驗！下一週，你將學習如何建立模型來解決另一種問題：分類問題（classification）。期待再見到你！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"2\" color=\"darkgreen\"><b>如果你想對「非評分」程式碼做實驗，請點此展開說明。</b></font></summary>\n",
    "    <p><i><b>重要提醒：為了避免影響自動評分，建議在你已經通過本作業之後再做以下調整。</b></i>\n",
    "    <ol>\n",
    "        <li> 在上方功能表中，點選「View」→「Cell Toolbar」→「Edit Metadata」</li>\n",
    "        <li> 在你想鎖定／解鎖的程式碼儲存格旁，按下「Edit Metadata」按鈕</li>\n",
    "        <li> 將該儲存格的 \"editable\" 屬性設為：\n",
    "            <ul>\n",
    "                <li> 設為 \"true\" 代表解鎖，可自由編輯 </li>\n",
    "                <li> 設為 \"false\" 代表鎖定，不可編輯 </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li> 完成後，在選單中點「View」→「Cell Toolbar」→「None」，關閉工具列顯示 </li>\n",
    "    </ol>\n",
    "    <p> 下圖是上述步驟的簡短示範： \n",
    "        <br>\n",
    "        <img src=\"https://lh3.google.com/u/0/d/14Xy_Mb17CZVgzVAgq7NCjMVBvSae3xO1\" align=\"center\" alt=\"unlock_cells.gif\">\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
