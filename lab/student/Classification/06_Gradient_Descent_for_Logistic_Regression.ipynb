{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03db750",
   "metadata": {},
   "source": [
    "# 邏輯迴歸的梯度下降（Gradient Descent）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94052b17",
   "metadata": {},
   "source": [
    "## 目標\n",
    "\n",
    "在這個 lab 中，你將會：\n",
    "\n",
    "- 更新（套用）邏輯迴歸的梯度下降流程。\n",
    "\n",
    "- 在熟悉的資料集上觀察並探索梯度下降的行為。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import copy, math\n",
    "import numpy as np\n",
    "try:\n",
    "    %matplotlib widget\n",
    "except:\n",
    "    %matplotlib inline\n",
    "    print(\"Colab not support matplotlib widget\")\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "#region 匯入資料\n",
    "def find_repo_root(marker=\"README.md\"):\n",
    "    cur = Path.cwd()\n",
    "    while cur != cur.parent:  # 防止無限迴圈，到達檔案系統根目錄就停\n",
    "        if (cur / marker).exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    return None\n",
    "\n",
    "\n",
    "def import_data_from_github():\n",
    "    import os, urllib.request, pathlib, shutil\n",
    "    \n",
    "    def isRunningInColab() -> bool:\n",
    "        return \"google.colab\" in sys.modules\n",
    "\n",
    "    def isRunningInJupyterLab() -> bool:\n",
    "        try:\n",
    "            import jupyterlab\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "        \n",
    "    def detect_env():\n",
    "        from IPython import get_ipython\n",
    "        if isRunningInColab():\n",
    "            return \"Colab\"\n",
    "        elif isRunningInJupyterLab():\n",
    "            return \"JupyterLab\"\n",
    "        elif \"notebook\" in str(type(get_ipython())).lower():\n",
    "            return \"Jupyter Notebook\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "    def get_utils_dir(env): \n",
    "        if env == \"Colab\": \n",
    "            if \"/content\" not in sys.path:\n",
    "                sys.path.insert(0, \"/content\")\n",
    "            return \"/content/utils\"\n",
    "        else:\n",
    "            return Path.cwd() / \"utils\"\n",
    "\n",
    "    env = detect_env()\n",
    "    UTILS_DIR = get_utils_dir(env)\n",
    "    REPO_DIR = \"Machine-Learning-Lab\"\n",
    "\n",
    "    #shutil.rmtree(UTILS_DIR, ignore_errors=True)\n",
    "    os.makedirs(UTILS_DIR, exist_ok=True)\n",
    "\n",
    "    BASE = f\"https://raw.githubusercontent.com/mz038197/{REPO_DIR}/main\"\n",
    "    urllib.request.urlretrieve(f\"{BASE}/utils/lab_utils_common_classification.py\", f\"{UTILS_DIR}/lab_utils_common_classification.py\")\n",
    "    urllib.request.urlretrieve(f\"{BASE}/utils/plt_quad_logistic.py\", f\"{UTILS_DIR}/plt_quad_logistic.py\")\n",
    "    urllib.request.urlretrieve(f\"{BASE}/utils/deeplearning.mplstyle\", f\"{UTILS_DIR}/deeplearning.mplstyle\")\n",
    "\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "\n",
    "if repo_root is None:\n",
    "    import_data_from_github()\n",
    "    repo_root = Path.cwd()\n",
    "    \n",
    "\n",
    "os.chdir(repo_root)\n",
    "print(f\"✅ 切換工作目錄至 {Path.cwd()}\")\n",
    "sys.path.append(str(repo_root)) if str(repo_root) not in sys.path else None\n",
    "print(f\"✅ 加入到系統路徑\")\n",
    "\n",
    "from utils.lab_utils_common_classification import  dlc, plot_data, plt_tumor_data, sigmoid, compute_cost_logistic\n",
    "from utils.plt_quad_logistic import plt_quad_logistic, plt_prob\n",
    "\n",
    "plt.style.use('utils/deeplearning.mplstyle')\n",
    "print(\"✅ 匯入模組及設定繪圖樣式\")\n",
    "#endregion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99407a8",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c4b34",
   "metadata": {},
   "source": [
    "## 資料集\n",
    "我們先使用與「決策邊界（Decision Boundary）」lab 相同的雙特徵資料集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbabade",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87290098",
   "metadata": {},
   "source": [
    "和之前一樣，我們會使用一個輔助函式來繪製資料。\n",
    "\n",
    "- 標籤為 $y=1$ 的資料點會以紅色叉叉表示\n",
    "- 標籤為 $y=0$ 的資料點會以藍色圓點表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc9b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
    "plot_data(X_train, y_train, ax)\n",
    "\n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "ax.set_ylabel('$x_1$', fontsize=12)\n",
    "ax.set_xlabel('$x_0$', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0af6d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aedb4db",
   "metadata": {},
   "source": [
    "## 邏輯迴歸的梯度下降\n",
    "\n",
    "回想梯度下降（Gradient Descent）演算法會使用梯度來更新參數：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中每一次迭代會同時更新所有 $w_j$（對所有 $j$），而梯度為：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- m 是資料集中訓練樣本的數量  \n",
    "\n",
    "- $f_{\\mathbf{w},b}(x^{(i)})$ 是模型的預測值，而 $y^{(i)}$ 是目標值\n",
    "\n",
    "- 對邏輯迴歸模型  \n",
    "    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
    "    $f_{\\mathbf{w},b}(x) = g(z)$  \n",
    "    其中 $g(z)$ 是 sigmoid 函數：  \n",
    "    $g(z) = \\frac{1}{1+e^{-z}}$   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78227ff3",
   "metadata": {},
   "source": [
    "### 梯度下降的實作\n",
    "梯度下降演算法的實作可以拆成兩個部分：\n",
    "- 實作上方公式 (1) 的迴圈。也就是下方的 `gradient_descent`；在選修與練習 lab 中通常會直接提供。\n",
    "- 計算目前梯度（上方公式 (2)、(3)）。也就是下方的 `compute_gradient_logistic`；在本週的 practice lab 中，你會被要求自行實作這部分。\n",
    "\n",
    "#### 計算梯度：程式碼說明\n",
    "這段程式會針對所有 $w_j$ 與 $b$ 實作上方公式 (2)、(3)。\n",
    "\n",
    "實作方式有很多種，這裡採用的流程如下：\n",
    "- 初始化用來累加 `dj_dw` 與 `dj_db` 的變數\n",
    "- 對每個樣本：\n",
    "\n",
    "    - 計算該樣本的誤差 $g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) - \\mathbf{y}^{(i)}$\n",
    "    - 對該樣本中的每個輸入值 $x_{j}^{(i)}$：\n",
    "\n",
    "        - 將誤差乘上輸入 $x_{j}^{(i)}$，並累加到 `dj_dw` 對應的元素（對應上方公式 2）\n",
    "    - 將誤差累加到 `dj_db`（對應上方公式 3）\n",
    "\n",
    "- 最後用樣本總數（m）去除 `dj_db` 與 `dj_dw`\n",
    "- 注意：在 numpy 中，$\\mathbf{x}^{(i)}$ 對應 `X[i,:]` 或 `X[i]`，而 $x_{j}^{(i)}$ 對應 `X[i,j]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df7c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "\n",
    "    # YOUR CODE END HERE\n",
    "\n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330476e6",
   "metadata": {},
   "source": [
    "請使用下方的 cell 來檢查梯度函數的實作是否正確。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db452845",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_tmp = np.array([0, 0, 0, 1, 1, 1])\n",
    "w_tmp = np.array([2.,3.])\n",
    "b_tmp = 1.\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\n",
    "print(f\"dj_db: {dj_db_tmp}\" )\n",
    "print(f\"dj_dw: {dj_dw_tmp.tolist()}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442471b4",
   "metadata": {},
   "source": [
    "**預期輸出**\n",
    "``` \n",
    "dj_db: 0.49861806546328574\n",
    "dj_dw: [0.498333393278696, 0.49883942983996693]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531ccd46",
   "metadata": {},
   "source": [
    "#### 梯度下降程式碼\n",
    "上方公式 (1) 的迴圈實作如下。請花點時間對照程式中的各個步驟與上方公式之間的關係。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b752f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd56fe3",
   "metadata": {},
   "source": [
    "接著在我們的資料集上執行梯度下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089683ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tmp  = np.zeros_like(X_train[0])\n",
    "b_tmp  = 0.\n",
    "alph = 0.1\n",
    "iters = 10000\n",
    "\n",
    "w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) \n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c40c0d",
   "metadata": {},
   "source": [
    "#### 繪製梯度下降的結果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(5,4))\n",
    "# plot the probability \n",
    "plt_prob(ax, w_out, b_out)\n",
    "\n",
    "# Plot the original data\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.set_xlabel(r'$x_0$')   \n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "plot_data(X_train,y_train,ax)\n",
    "\n",
    "# Plot the decision boundary\n",
    "x0 = -b_out/w_out[0]\n",
    "x1 = -b_out/w_out[1]\n",
    "ax.plot([0,x0],[x1,0], c=dlc[\"dlblue\"], lw=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac94cb53",
   "metadata": {},
   "source": [
    "在上圖中：\n",
    " - 陰影顏色代表 $y=1$ 的機率（也就是套用決策邊界之前的機率輸出）\n",
    " - 決策邊界是「機率 = 0.5」的位置所形成的那條線\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c7f34",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec5e388",
   "metadata": {},
   "source": [
    "## 另一個資料集\n",
    "我們回到單一變數的資料集。當模型只有兩個參數 $w$、$b$ 時，我們可以用等高線圖（contour plot）把成本函數畫出來，更直覺地了解梯度下降到底在做什麼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199418a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([0., 1, 2, 3, 4, 5])\n",
    "y_train = np.array([0,  0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a894000",
   "metadata": {},
   "source": [
    "和之前一樣，我們會使用一個輔助函式來繪製資料。\n",
    "\n",
    "- 標籤為 $y=1$ 的資料點會以紅色叉叉表示\n",
    "- 標籤為 $y=0$ 的資料點會以藍色圓點表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22502c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(4,3))\n",
    "plt_tumor_data(x_train, y_train, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380cf670",
   "metadata": {},
   "source": [
    "在下方的圖中，請嘗試：\n",
    "- 在右上角的等高線圖內點擊，以改變 $w$ 與 $b$。\n",
    "\n",
    "    - 變更可能需要 1～2 秒才會反映\n",
    "    - 注意左上角圖中的 cost 會跟著改變\n",
    "    - 注意 cost 是由每個樣本的 loss 累積而來（垂直虛線）\n",
    "\n",
    "- 點擊橘色按鈕來執行梯度下降。\n",
    "\n",
    "    - 注意 cost 會穩定下降（等高線圖與 cost 圖顯示的是 log(cost)）\n",
    "    - 在等高線圖中再次點擊會重設模型，方便你重新跑一次\n",
    "\n",
    "- 若要重設整張圖，請重新執行這個 cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_range = np.array([-1, 7])\n",
    "b_range = np.array([1, -14])\n",
    "quad = plt_quad_logistic( x_train, y_train, w_range, b_range )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b188039e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06908c5d",
   "metadata": {},
   "source": [
    "## 恭喜！\n",
    "\n",
    "你已經：\n",
    "\n",
    "- 檢視了邏輯斯迴歸梯度計算的公式與程式實作\n",
    "\n",
    "- 並且在以下情境中實際使用了這些程式：\n",
    "\n",
    "    - 探索單一變數資料集\n",
    "    \n",
    "    - 探索雙變數資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4653391b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
